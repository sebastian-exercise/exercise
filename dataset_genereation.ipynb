{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the code used for generating different tables used by the rest of the noteeboks. In most cases, such tables are just samples of the original tables in the Challenge dataset, but some of them are bit more involved. Using these generated tables allows for lower running times in the rest of the notebooks. \n",
    "\n",
    "Having this code in a separate notebook allows for cleaner notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and main params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_json_to_csv(json_path, csv_path, keep_cols = None, smp_frac = None, \n",
    "                        chunksize = 100000, encoding = None, random_state = None):\n",
    "    \"\"\" Reads a json file and saves it as csv, optionally filtering rows and columns. \n",
    "        \n",
    "    Parameters\n",
    "    ----------\n",
    "    json_path : string\n",
    "        path to the json file\n",
    "    csv_path : string\n",
    "        path to the csv file\n",
    "    keep_cols : list of strings, optional\n",
    "        column names of the columns to keep\n",
    "    smp_frac : float, optional\n",
    "        fraction of rows from the json file to keep\n",
    "    chunksize : int, optional\n",
    "        number of rows to read from the json file per iteration\n",
    "    encoding: string, optional\n",
    "        econding to use in the csv file\n",
    "    random_state: int\n",
    "        seed for sampling the json file \n",
    "    \"\"\"\n",
    "    reader = pd.read_json(json_path, lines=True, chunksize=chunksize)\n",
    "    df = pd.DataFrame()\n",
    "    for chunk in reader:\n",
    "        if keep_cols is not None:\n",
    "            chunk = chunk[keep_cols]\n",
    "        if smp_frac is not None:\n",
    "            chunk = chunk.sample(frac=smp_frac, random_state=random_state)\n",
    "        print('Concatenating new chunk...')\n",
    "        df = pd.concat([df, chunk])\n",
    "        print('Read {} rows'.format(df.shape[0]))\n",
    "        \n",
    "    print('Saving CSV...')\n",
    "    df.to_csv(csv_path, index=False, encoding=encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenating new chunk...\n",
      "Read 500000 rows\n",
      "Concatenating new chunk...\n",
      "Read 1000000 rows\n",
      "Concatenating new chunk...\n",
      "Read 1500000 rows\n",
      "Concatenating new chunk...\n",
      "Read 2000000 rows\n",
      "Concatenating new chunk...\n",
      "Read 2500000 rows\n",
      "Concatenating new chunk...\n",
      "Read 3000000 rows\n",
      "Concatenating new chunk...\n",
      "Read 3500000 rows\n",
      "Concatenating new chunk...\n",
      "Read 4000000 rows\n",
      "Concatenating new chunk...\n",
      "Read 4500000 rows\n",
      "Concatenating new chunk...\n",
      "Read 5000000 rows\n",
      "Concatenating new chunk...\n",
      "Read 5500000 rows\n",
      "Concatenating new chunk...\n",
      "Read 5996996 rows\n",
      "Saving CSV...\n"
     ]
    }
   ],
   "source": [
    "reviews_path = '../data/yelp_academic_dataset_review.json'\n",
    "generated_reviews_path = '../data/reviews.csv'\n",
    "keep_cols = ['business_id', 'date', 'stars', 'user_id'] \n",
    "extract_json_to_csv(reviews_path, generated_reviews_path, keep_cols, chunksize = 500000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenating new chunk...\n",
      "Read 10000 rows\n",
      "Concatenating new chunk...\n",
      "Read 20000 rows\n",
      "Concatenating new chunk...\n",
      "Read 30000 rows\n",
      "Concatenating new chunk...\n",
      "Read 40000 rows\n",
      "Concatenating new chunk...\n",
      "Read 50000 rows\n",
      "Concatenating new chunk...\n",
      "Read 60000 rows\n",
      "Concatenating new chunk...\n",
      "Read 70000 rows\n",
      "Concatenating new chunk...\n",
      "Read 80000 rows\n",
      "Concatenating new chunk...\n",
      "Read 90000 rows\n",
      "Concatenating new chunk...\n",
      "Read 100000 rows\n",
      "Concatenating new chunk...\n",
      "Read 110000 rows\n",
      "Concatenating new chunk...\n",
      "Read 120000 rows\n",
      "Concatenating new chunk...\n",
      "Read 130000 rows\n",
      "Concatenating new chunk...\n",
      "Read 140000 rows\n",
      "Concatenating new chunk...\n",
      "Read 150000 rows\n",
      "Concatenating new chunk...\n",
      "Read 151817 rows\n",
      "Saving CSV...\n"
     ]
    }
   ],
   "source": [
    "users_path = '../data/yelp_academic_dataset_user.json'\n",
    "generated_users_path = '../data/users.csv'\n",
    "keep_cols = ['user_id', 'friends'] \n",
    "extract_json_to_csv(users_path, generated_users_path, keep_cols, smp_frac = 0.1,\n",
    "                    chunksize = 100000, encoding='utf-8', random_state=random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Businesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenating new chunk...\n",
      "Read 188593 rows\n",
      "Saving CSV...\n"
     ]
    }
   ],
   "source": [
    "businesses_path = '../data/yelp_academic_dataset_business.json'\n",
    "generated_businesses_path = '../data/businesses.csv'\n",
    "extract_json_to_csv(businesses_path, generated_businesses_path, chunksize = 500000, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell creates a checkings CSV with just two columns (apart from business_id), corresponding to the number of checkins on weekends and on working days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk\n",
      "Concatenating new chunk...\n",
      "Read 20000 rows\n",
      "Processing chunk\n",
      "Concatenating new chunk...\n",
      "Read 40000 rows\n",
      "Processing chunk\n",
      "Concatenating new chunk...\n",
      "Read 60000 rows\n",
      "Processing chunk\n",
      "Concatenating new chunk...\n",
      "Read 80000 rows\n",
      "Processing chunk\n",
      "Concatenating new chunk...\n",
      "Read 100000 rows\n",
      "Processing chunk\n",
      "Concatenating new chunk...\n",
      "Read 120000 rows\n",
      "Processing chunk\n",
      "Concatenating new chunk...\n",
      "Read 140000 rows\n",
      "Processing chunk\n",
      "Concatenating new chunk...\n",
      "Read 157075 rows\n"
     ]
    }
   ],
   "source": [
    "checkins_path = '../data/yelp_academic_dataset_checkin.json'\n",
    "generated_checkins_path = '../data/checkins.csv'\n",
    "\n",
    "\n",
    "reader = pd.read_json(checkins_path, lines=True, chunksize=20000)\n",
    "df = pd.DataFrame()\n",
    "for chunk in reader:\n",
    "    print('Processing chunk')\n",
    "    chunk = pd.concat([chunk.drop(['time'], axis=1), chunk['time'].apply(pd.Series)], axis=1).fillna(0)\n",
    "    weekend_cols = [col for col in chunk.drop('business_id', axis=1).columns if \\\n",
    "                    (col.split('-')[0] in ['Sat', 'Sun'])]\n",
    "    week_cols = [col for col in chunk.drop('business_id', axis=1).columns if \\\n",
    "                 (col.split('-')[0] in ['Mon', 'Tue', 'Wed', 'Thu', 'Fri'])]\n",
    "    chunk['weekends'] = chunk[weekend_cols].sum(axis=1)\n",
    "    chunk['week'] = chunk[week_cols].sum(axis=1)\n",
    "    chunk = chunk[['business_id', 'weekends', 'week']]\n",
    "    print('Concatenating new chunk...')\n",
    "    df = pd.concat([df, chunk])\n",
    "    print('Read {} rows'.format(df.shape[0]))\n",
    "df.to_csv(generated_checkins_path, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Friends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 3 cells below create a handy mapping between friends. The generated table is based on a sample from the generated users table (run the cell under _Users_ first if you are going to run the cells below). First, users with less than a certain number of friends are filtered out. Then, a number of users are randomly sampled. The final table contains two columns: 'user_id' and 'friend_id'. A row [user_id, friend_id] is present in the final table if and only if user_id belongs to the sampled table and user_id has friend_id among her friends. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Params:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_users_path = '../data/users.csv'\n",
    "friends_path = '../data/friends.csv'\n",
    "\n",
    "# Min number of friends. Users with less friends than this value are filtered out\n",
    "min_n_friends = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read users, preprocess friends column and filter out users with less than min_n_friends friends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = pd.read_csv(generated_users_path)\n",
    "users = users[users.friends != 'None'].copy()\n",
    "users['friends'] = users.friends.apply(lambda x: [i.strip() for i in x.split(',')])\n",
    "users = users[users.friends.apply(lambda x: len(x) >= 20)].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explode list of friends and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Efficent function for exploding a list column, creating a new row for each element in list\n",
    "# Source: https://github.com/pandas-dev/pandas/issues/10511\n",
    "def unlistify(df, column):\n",
    "    matches = [i for i,n in enumerate(df.columns) if n==column]\n",
    "\n",
    "    if len(matches)==0:\n",
    "        raise Exception('Failed to find column named ' + column +'!')\n",
    "    if len(matches)>1:\n",
    "        raise Exception('More than one column named ' + column +'!')\n",
    "\n",
    "    col_idx = matches[0]\n",
    "    \n",
    "    # Helper function to expand and repeat the column col_idx\n",
    "    def fnc(d):\n",
    "        row = list(d.values[0])\n",
    "        bef = row[:col_idx]\n",
    "        aft = row[col_idx+1:]\n",
    "        col = row[col_idx]\n",
    "        z = [bef + [c] + aft for c in col]\n",
    "        return pd.DataFrame(z)\n",
    "    \n",
    "    col_idx += len(df.index.shape) # Since we will push reset the index\n",
    "    index_names = list(df.index.names)\n",
    "    column_names = list(index_names) + list(df.columns)\n",
    "    return (df\n",
    "            .reset_index()\n",
    "            .groupby(level=0,as_index=0)\n",
    "            .apply(fnc)\n",
    "            .rename(columns = lambda i :column_names[i])\n",
    "            .set_index(index_names)\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = unlistify(users, 'friends')\n",
    "users.columns = ['user_id', 'friend_id']\n",
    "users.to_csv(friends_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# friends_path = '../data/friends.csv'\n",
    "# users = users.friends.apply(pd.Series) \\\n",
    "#              .stack() \\\n",
    "#              .reset_index(level=1, drop=True) \\\n",
    "#              .to_frame('friend_id') \\\n",
    "#              .join(users.drop('friends', axis=1)) \\\n",
    "#              .reset_index(level=0, drop=True)\n",
    "# users = users[users.friend_id != '']\n",
    "# users.to_csv(friends_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
