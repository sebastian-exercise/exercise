{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the code used for generating different tables used by the rest of the noteeboks. In most cases, such tables are just samples of the original tables in the Challenge dataset, but some of them are bit more involved. Using these generated tables allows for lower running times in the rest of the notebooks. \n",
    "\n",
    "Having this code in a separate notebook allows for cleaner notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and main params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = '/media/sebastian/926CA79B6CA7791D/trabajo/busqueda/research/NewYorker/project/data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_json_to_csv(json_path, csv_path, keep_cols=None, chunksize = 100000, encoding=None):\n",
    "    reader = pd.read_json(json_path, lines=True, chunksize=chunksize)\n",
    "    df = pd.DataFrame()\n",
    "    for chunk in reader:\n",
    "        if keep_cols is not None:\n",
    "            chunk = chunk[keep_cols]\n",
    "        print('Concatenating new chunk...')\n",
    "        df = pd.concat([df, chunk])\n",
    "        print('Read {} rows'.format(df.shape[0]))\n",
    "        \n",
    "    print('Saving CSV...')\n",
    "    df.to_csv(csv_path, index=False, encoding=encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_path = base_path + 'yelp_academic_dataset_review.json'\n",
    "generated_reviews_path = base_path + 'generated/reviews.csv'\n",
    "keep_cols = ['business_id', 'date', 'stars', 'user_id'] \n",
    "# extract_json_to_csv(reviews_path, generated_reviews_path, keep_cols, chunksize = 500000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_path = base_path + 'yelp_academic_dataset_user.json'\n",
    "generated_users_path = base_path + 'generated/users.csv'\n",
    "keep_cols = ['user_id', 'friends'] \n",
    "# extract_json_to_csv(users_path, generated_users_path, keep_cols, chunksize = 500000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Businesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenating new chunk...\n",
      "Read 188593 rows\n",
      "Saving CSV...\n"
     ]
    }
   ],
   "source": [
    "businesses_path = base_path + 'yelp_academic_dataset_business.json'\n",
    "generated_businesses_path = base_path + 'generated/businesses.csv'\n",
    "keep_cols = ['user_id', 'friends'] \n",
    "extract_json_to_csv(businesses_path, generated_businesses_path, chunksize = 500000, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell creates a checkings CSV with just two columns (apart from business_id), corresponding to the number of checkins on weekends and on working days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk\n",
      "Concatenating new chunk...\n",
      "Read 20000 rows\n",
      "Processing chunk\n",
      "Concatenating new chunk...\n",
      "Read 40000 rows\n",
      "Processing chunk\n",
      "Concatenating new chunk...\n",
      "Read 60000 rows\n",
      "Processing chunk\n",
      "Concatenating new chunk...\n",
      "Read 80000 rows\n",
      "Processing chunk\n",
      "Concatenating new chunk...\n",
      "Read 100000 rows\n",
      "Processing chunk\n",
      "Concatenating new chunk...\n",
      "Read 120000 rows\n",
      "Processing chunk\n",
      "Concatenating new chunk...\n",
      "Read 140000 rows\n",
      "Processing chunk\n",
      "Concatenating new chunk...\n",
      "Read 157075 rows\n"
     ]
    }
   ],
   "source": [
    "checkins_path = base_path + 'yelp_academic_dataset_checkin.json'\n",
    "generated_checkins_path = base_path + 'generated/checkins.csv'\n",
    "\n",
    "\n",
    "reader = pd.read_json(checkins_path, lines=True, chunksize=20000)\n",
    "df = pd.DataFrame()\n",
    "for chunk in reader:\n",
    "    print('Processing chunk')\n",
    "    chunk = pd.concat([chunk.drop(['time'], axis=1), chunk['time'].apply(pd.Series)], axis=1).fillna(0)\n",
    "    weekend_cols = [col for col in chunk.drop('business_id', axis=1).columns if \\\n",
    "                    (col.split('-')[0] in ['Sat', 'Sun'])]\n",
    "    week_cols = [col for col in chunk.drop('business_id', axis=1).columns if \\\n",
    "                 (col.split('-')[0] in ['Mon', 'Tue', 'Wed', 'Thu', 'Fri'])]\n",
    "    chunk['weekends'] = chunk[weekend_cols].sum(axis=1)\n",
    "    chunk['week'] = chunk[week_cols].sum(axis=1)\n",
    "    chunk = chunk[['business_id', 'weekends', 'week']]\n",
    "    print('Concatenating new chunk...')\n",
    "    df = pd.concat([df, chunk])\n",
    "    print('Read {} rows'.format(df.shape[0]))\n",
    "df.to_csv(generated_checkins_path, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Friends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 3 cells below create a handy mapping between friends. The generated table is based on a sample from the generated users table (run the cell under _Users_ first if you are going to run the cells below). First, users with less than a certain number of friends are filtered out. Then, a number of users are randomly sampled. The final table contains two columns: 'user_id' and 'friend_id'. A row [user_id, friend_id] is present in the final table if and only if user_id belongs to the sampled table and user_id has friend_id among her friends. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Params:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of users to sample from the users table \n",
    "sample = 20000\n",
    "\n",
    "# Min number of friends. Users with less friends than this value are filtered out\n",
    "min_n_friends = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read users, preprocess friends column, sample and filter out users with less than min_n_friends friends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = pd.read_csv(generated_users_path)\n",
    "users = users[users.friends != 'None'].copy()\n",
    "users['friends'] = users.friends.apply(lambda x: [i.strip() for i in x.split(',')])\n",
    "users = users[users.friends.apply(lambda x: len(x) >= 20)].copy()\n",
    "users = users.sample(20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explode list of friends and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "friends_path = base_path + 'generated/friends.csv'\n",
    "friends = users.friends.apply(pd.Series) \\\n",
    "               .stack() \\\n",
    "               .reset_index(level=1, drop=True) \\\n",
    "               .to_frame('friend_id') \\\n",
    "               .join(users) \\\n",
    "               .drop('friends', axis=1) \\\n",
    "               .reset_index(level=0, drop=True)\n",
    "friends = friends[friends.friend_id != '']\n",
    "friends.to_csv(friends_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
